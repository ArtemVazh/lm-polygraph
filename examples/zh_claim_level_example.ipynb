{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2302d3-10a1-4427-84e1-3f39913142f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5465947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from lm_polygraph.utils.model import WhiteboxModel\n",
    "from lm_polygraph.estimators import *\n",
    "from lm_polygraph.stat_calculators import *\n",
    "from lm_polygraph.utils.openai_chat import OpenAIChat\n",
    "from lm_polygraph.utils.deberta import Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7ae864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruixing/.conda/envs/poly/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "bloomz=\"bigscience/bloomz-560m\"\n",
    "yi6bchat=\"01-ai/Yi-6B-Chat\"\n",
    "yi6b=\"01-ai/Yi-6B\"\n",
    "t5base='google-t5/t5-base'\n",
    "model = WhiteboxModel.from_pretrained(yi6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802f4378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = [\"请介绍一下曹操。\"]\n",
    "stat = {}\n",
    "\n",
    "# MBZ Budget\n",
    "os.environ[\"OPENAI_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "for calculator in [\n",
    "    GreedyProbsCalculator(),\n",
    "    EntropyCalculator(),\n",
    "    GreedyLMProbsCalculator(),\n",
    "    # ClaimsExtractorZH(OpenAIChat(\"gpt-4o\")),\n",
    "]:\n",
    "    stat.update(calculator(stat, texts, model))\n",
    "\n",
    "claim_extractor=ClaimsExtractor(OpenAIChat(\"gpt-4o\"))\n",
    "stat.update(claim_extractor(stat, texts, model,language=\"zh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fade0e70-9077-4ccd-beb2-6b9b36cc954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_texts', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder', 'entropy', 'greedy_lm_log_probs', 'greedy_lm_log_likelihoods', 'claims', 'claim_texts_concatenated', 'claim_input_texts_concatenated'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "505008d3-3e93-4612-91f3-f90e1eab9706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Claim(claim_text='曹操（155年－220年3月15日）。', sentence='曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人', aligned_tokens=[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]),\n",
       "  Claim(claim_text='曹操字孟德。', sentence='曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人', aligned_tokens=[1, 19, 20, 21]),\n",
       "  Claim(claim_text='曹操小字阿瞒。', sentence='曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人', aligned_tokens=[1, 23, 24, 25, 26]),\n",
       "  Claim(claim_text='曹操是沛国谯县（今安徽亳州）人。', sentence='曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人', aligned_tokens=[1, 28, 29]),\n",
       "  Claim(claim_text='东汉末年杰出的政治家。', sentence='东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人', aligned_tokens=[44, 45, 46, 47, 48, 49, 50, 51]),\n",
       "  Claim(claim_text='东汉末年杰出的军事家。', sentence='东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人', aligned_tokens=[44, 45, 46, 47, 48, 49, 53, 54]),\n",
       "  Claim(claim_text='东汉末年杰出的文学家。', sentence='东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人', aligned_tokens=[44, 45, 46, 47, 48, 49, 56, 57]),\n",
       "  Claim(claim_text='东汉末年杰出的书法家。', sentence='东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人', aligned_tokens=[44, 45, 46, 47, 48, 49, 59, 60]),\n",
       "  Claim(claim_text='三国中曹魏政权的奠基人。', sentence='东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人', aligned_tokens=[62, 63, 64, 65, 66, 67, 68, 69, 70])]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde52623-aac2-4315-9e32-a1fc93e0de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人。东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人。\n",
      "曹操在东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\", stat[\"greedy_texts\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99e12fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "曹操（155年－220年3月15日），字孟德，小字阿瞒，沛国谯县（今安徽亳州）人。东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠基人。\n",
      "曹操在东汉末年杰出的政治家、军事家、文学家、书法家，三国中曹魏政权的奠\n",
      "\n",
      "claim: 曹操（155年－220年3月15日）。\n",
      "aligned tokens: [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "claim: 曹操字孟德。\n",
      "aligned tokens: [1, 19, 20, 21]\n",
      "\n",
      "claim: 曹操小字阿瞒。\n",
      "aligned tokens: [1, 23, 24, 25, 26]\n",
      "\n",
      "claim: 曹操是沛国谯县（今安徽亳州）人。\n",
      "aligned tokens: [1, 28, 29]\n",
      "\n",
      "claim: 东汉末年杰出的政治家。\n",
      "aligned tokens: [44, 45, 46, 47, 48, 49, 50, 51]\n",
      "\n",
      "claim: 东汉末年杰出的军事家。\n",
      "aligned tokens: [44, 45, 46, 47, 48, 49, 53, 54]\n",
      "\n",
      "claim: 东汉末年杰出的文学家。\n",
      "aligned tokens: [44, 45, 46, 47, 48, 49, 56, 57]\n",
      "\n",
      "claim: 东汉末年杰出的书法家。\n",
      "aligned tokens: [44, 45, 46, 47, 48, 49, 59, 60]\n",
      "\n",
      "claim: 三国中曹魏政权的奠基人。\n",
      "aligned tokens: [62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\", stat[\"greedy_texts\"][0])\n",
    "print()\n",
    "for claim in stat[\"claims\"][0]:\n",
    "    print(\"claim:\", claim.claim_text)\n",
    "    print(\"aligned tokens:\", claim.aligned_tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6750525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.059152,\n",
       "  1.1362455,\n",
       "  2.9324822,\n",
       "  1.4121381,\n",
       "  0.7077023,\n",
       "  0.69397306,\n",
       "  0.7427101,\n",
       "  0.69853115,\n",
       "  0.5728888]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum Claim Probability\n",
    "max_prob = MaximumClaimProbability()\n",
    "max_prob(stat)  # Uncertainty for each claim, the higher, the less certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85be9f76-3f50-4b32-8867-45849fab8eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1372768,\n",
       "  0.28406137,\n",
       "  0.5864965,\n",
       "  0.4707127,\n",
       "  0.088462785,\n",
       "  0.08674663,\n",
       "  0.092838764,\n",
       "  0.087316394,\n",
       "  0.06365431]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perlexity\n",
    "perlexity_claim = PerplexityClaim()\n",
    "perlexity_claim (stat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7b4a46a-37e9-4a8d-b583-c43cb0f94692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.67348e-05,\n",
       "  7.67348e-05,\n",
       "  7.67348e-05,\n",
       "  7.67348e-05,\n",
       "  2.765898e-05,\n",
       "  2.765898e-05,\n",
       "  2.765898e-05,\n",
       "  2.765898e-05,\n",
       "  1.8441091e-05]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum Token Entropy\n",
    "max_token_ent = MaxTokenEntropyClaim()\n",
    "max_token_ent(stat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f118841-c40b-4d04-b06f-da7a6b1c287c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-72.85801413972513,\n",
       "  -30.04430177868926,\n",
       "  -47.79385693371296,\n",
       "  -26.227787380106747,\n",
       "  -107.58818900730694,\n",
       "  -107.45839975989657,\n",
       "  -108.33176562050357,\n",
       "  -106.02369641518453,\n",
       "  -128.93925437084226]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pointwise Mutual Information\n",
    "pmi = PointwiseMutualInformationClaim()\n",
    "pmi (stat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ffc38-e905-42eb-aa48-5dee774afed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTrue\n",
    "ptrue_claim = PTrueClaim()\n",
    "ptrue_claim (stat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42d9982a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.7113813961415233,\n",
       "  -0.8446114377545357,\n",
       "  -0.2803199309110498,\n",
       "  -0.7791504012475273,\n",
       "  -0.5517237721010816,\n",
       "  -0.557831080274213,\n",
       "  -0.5316904995544154,\n",
       "  -0.5553218365419984,\n",
       "  -0.7292863406843685]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Claim Conditional Probability\n",
    "for calculator in [\n",
    "    GreedyAlternativesNLICalculator(Deberta(deberta_path=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"))\n",
    "]:\n",
    "    stat.update(calculator(stat, texts, model))\n",
    "\n",
    "ccp = ClaimConditionedProbabilityClaim()\n",
    "ccp (stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d1a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.7235015435870603,\n",
       "  -0.8481761220949592,\n",
       "  -0.39506225276896145,\n",
       "  -0.7651305626056635,\n",
       "  -0.5732555949369494,\n",
       "  -0.5790878572436547,\n",
       "  -0.5517374369245559,\n",
       "  -0.5765155444379375,\n",
       "  -0.6967716564740467]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for calculator in [\n",
    "    GreedyAlternativesFactPrefNLICalculator(Deberta(deberta_path=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"))\n",
    "]:\n",
    "    stat.update(calculator(stat, texts, model))\n",
    "\n",
    "ccp_no_cxt=ClaimConditionedProbabilityClaim(nli_context=\"fact_pref\")\n",
    "ccp_no_cxt (stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e852a2cc-b319-490e-8ae7-e258dd4b41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_polygraph.generation_metrics.openai_fact_check import OpenAIFactCheck\n",
    "chinese_checker = OpenAIFactCheck('gpt-4o')\n",
    "chatgpt_response = chinese_checker(stat, None, None, language=\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0381fff-e2d2-43b1-b830-edec434d2f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, nan, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac0849-cb89-4a66-a63f-1dbbe6e58b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
