# Output directory where logs, managers and other evaluation artifacts will be 
# placed.
hydra:
  run:
    dir: ${cache_path}/${task}/${model}/${dataset}/${now:%Y-%m-%d}/${now:%H-%M-%S}

cache_path: ./workdir/output
save_path: '${hydra:run.dir}'

# Device where model will be placed for inference.
device: cpu

# Task name, used only in output directory path.
task: ats

# Whether to stop execution if exception is raised in one of estimators.
# If set to false, corresponding estimator will be removed from further
# evaluation, and exception's traceback put into stderr.
ignore_exceptions: false

# List of random seeds. Evaluation will be performed sequentially for each
# of the specified seeds.
seed:
    - 1

# ============================================================
#                       DATASET OPTIONS
# ============================================================

# Either HuggingFace datset name, or absolute path to csv containing data.
dataset: aeslc
# Name of feature in the dataset that contains input text.
text_column: email_body
# Name of fetature in the dataset that contains label text.
label_column: subject_line
# Prompt for the model under evaluation. Input text will be interpolated
# instead of {text} placeholder.
prompt: "Write a short subject line for the email. Output only the subject line itself.\n\nEmail:\n{text}\n\nSubject line:\n"
# Name of dataset split used as a trainset (for UE methods that need to be fitted.
train_split: train
# Name of dataset split for inference and uncertainty estimation.
eval_split: test
# Will be passed to HF load_dataset(). Ignored if dataset is loaded from csv.
load_from_disk: false
# Maximum length of generated sequence, will be passed to model's generate()
max_new_tokens: 31

# Name of train dataset
train_dataset: null
# If train dataset is omitted and this option is set to true, test dataset will
# be split in two parts, one of which will be used for fitting and other for 
# evaluation.
# IMPORTANT: if eval_split is set to true, and train_test split is also set to
# true, train and eval datasets will be the same!
train_test_split: false
# Size of the test split. Ignored if train_test_split is not true.
test_split_size: 1

# Options for background train data used for Relative Mahalonobis Distance approach.
# Behaves similarly to other datasets options.
background_train_dataset: allenai/c4
background_train_dataset_text_column: text
background_train_dataset_label_column: url
background_train_dataset_data_files: en/c4-train.00000-of-01024.json.gz
background_load_from_disk: false

# Size of dataset subsamples if whole dataset is too big. Subsampling is
# performed randomly based on the random seed set in this config.
subsample_background_train_dataset: 1000
subsample_train_dataset: 1000
subsample_eval_dataset: -1

# ============================================================
#                       MODEL OPTIONS
# ============================================================

# HF model name
model: databricks/dolly-v2-3b
# Passed to model's kwargs and forwarded to from_pretrained() if
# the model requires authentication on HF Hub
use_auth_token:
# Batch size to use for model under evaluation.
batch_size: 1
# Batch size for DeBerta model used to estimate sequence similarity in
# black-box UE methods.
deberta_batch_size: 10

# ============================================================
#                       ENSEMBLE OPTIONS
# ============================================================

# Should we also load ensemble?
ensemble: false
# Type of ensemble to use: product-of-expectations or expectation-of-products.
# For detailed treatment on types of LM ensembles: https://openreview.net/pdf?id=jN5y-zb5Q7m\
# Valid options are: pe or ep
ens_type: 
# Should ensemble be based on MC-Dropout method?
mc: false
# If yes, please specify seeds for dropout masks of each ensemble member.
# Number of seeds determines size of ensemble.
mc_seeds: []
# Dropout rate for MC-Dropout ensemble.
dropout_rate:

# ============================================================
#                      UE Options 
# ============================================================

# Whether to use default list of density-based UE approaches.
#
# Default density-based estimators for seq2seq models are:
# - MahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
# - MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
# - RelativeMahalanobisDistanceSeq(
# -     "encoder", parameters_path=parameters_path
# - ),
# - RelativeMahalanobisDistanceSeq(
# -     "decoder", parameters_path=parameters_path
# - ),
# - RDESeq("encoder", parameters_path=parameters_path),
# - RDESeq("decoder", parameters_path=parameters_path),
# - PPLMDSeq("encoder", md_type="MD", parameters_path=parameters_path),
# - PPLMDSeq("encoder", md_type="RMD", parameters_path=parameters_path),
# - PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
# - PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
#
# Default density-based estimators for causal models are:
# - MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
# - RelativeMahalanobisDistanceSeq(
# -     "decoder", parameters_path=parameters_path
# - ),
# - RDESeq("decoder", parameters_path=parameters_path),
# - PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
# - PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
#
use_density_based_ue: true

# Whether to use default list of sequence-level UE approaches.
# Default seq-level estimators are:
# - MaximumSequenceProbability(),
# - Perplexity(),
# - MeanTokenEntropy(),
# - MeanPointwiseMutualInformation(),
# - MeanConditionalPointwiseMutualInformation(),
# - PTrue(),
# - PTrueSampling(),
# - MonteCarloSequenceEntropy(),
# - MonteCarloNormalizedSequenceEntropy(),
# - LexicalSimilarity(metric="rouge1"),
# - LexicalSimilarity(metric="rouge2"),
# - LexicalSimilarity(metric="rougeL"),
# - LexicalSimilarity(metric="BLEU"),
# - NumSemSets(),
# - EigValLaplacian(similarity_score="NLI_score", affinity="entail"),
# - EigValLaplacian(similarity_score="NLI_score", affinity="contra"),
# - EigValLaplacian(similarity_score="Jaccard_score"),
# - DegMat(similarity_score="NLI_score", affinity="entail"),
# - DegMat(similarity_score="NLI_score", affinity="contra"),
# - DegMat(similarity_score="Jaccard_score"),
# - Eccentricity(similarity_score="NLI_score", affinity="entail"),
# - Eccentricity(similarity_score="NLI_score", affinity="contra"),
# - Eccentricity(similarity_score="Jaccard_score"),
# - SemanticEntropy(),
use_seq_ue: true

# Whether to use default list of token-level UE approaches.
# Currently does nothing, as token-level methods are not implemented.
#
use_tok_ue: false

# Whether to use default list of ensemble-based UE approaches.
# Default ensemble estimators depend on type of ensemble.
#
# For both types common estimators are:
# - PETtu(),
# - PETdu(),
# - PETmi(),
# - PETrmi(),
# - PETepkl(),
# - PETent5(),
# - PETent10(),
# - PETent15(),
# - EPTtu(),
# - EPTdu(),
# - EPTmi(),
# - EPTrmi(),
# - EPTepkl(),
# - EPTent5(),
# - EPTent10(),
# - EPTent15(),
#
# EP-specific methods are: 
# - EPStu()
# - EPSrmi()
#
# EP-specific methods are:
# - PEStu()
# - PESrmi()
#
use_ens_ue: false

# Generation quality metrics to use. Default list is used if not set or set to null
# Should be a subset of this default list.
# Default generation metrics:
# - RougeMetric("rouge1"),
# - RougeMetric("rouge2"),
# - RougeMetric("rougeL"),
# - BartScoreSeqMetric("rh"),
# - BertScoreMetric(),
# - SbertMetric(),
# - AccuracyMetric(),
#
# Example of specifying other subset:
# generation_metrics:
#   - name: RougeMetric
#     args: ["rouge1"]
#   - name: AccuracyMetric
generation_metrics: null

# Examples of providing additional UE methods:
# additional_estimators: {
#   'lm_polygraph.estimators.perplexity': ['Perplexity'],
#   'lm_polygraph.estimators.eig_val_laplacian': ['EigValLaplacian']
# }
# additional_estimators_kwargs: {
#   'Perplexity': {},
#   'EigValLaplacian': {'similarity_score': 'NLI_score', 'affinity': 'entail'}
# }
