#!/usr/bin/env python3

import hydra
import importlib
import os
import torch
import transformers
import argparse
from pathlib import Path
import json

import logging

log = logging.getLogger()


from lm_polygraph.utils.manager import UEManager
from lm_polygraph.utils.dataset import Dataset
from lm_polygraph.utils.model import WhiteboxModel
from lm_polygraph.utils.processor import Logger
from lm_polygraph.generation_metrics import (
    RougeMetric,
    WERTokenwiseMetric,
    BartScoreSeqMetric,
    ModelScoreSeqMetric,
    ModelScoreTokenwiseMetric, 
    AccuracyMetric
)
from lm_polygraph.estimators import *
from lm_polygraph.ue_metrics import (
    ReversedPairsProportion,
    PredictionRejectionArea,
    RiskCoverageCurveAUC,
)


DENSITY_BASED_ESTIMATORS = [
    "MahalanobisDistanceSeq",
    "RelativeMahalanobisDistanceSeq",
    "RDESeq",
    "PPLMDSeq",
]


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("--estimator", type=str, required=True)
    parser.add_argument("--estimator_kwargs", type=json.loads, required=True)
    parser.add_argument("--estimator_module", type=str, default=None)
    
    parser.add_argument("--additional_estimators", type=json.loads, default={})

    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--train_dataset", type=str)
    parser.add_argument("--background_train_dataset", type=str)
    parser.add_argument("--text_column", type=str, default="question")
    parser.add_argument("--label_column", type=str, default="answer")
    parser.add_argument("--prompt", type=str, default='')

    parser.add_argument("--model", type=str, required=True)
    parser.add_argument(
        "--ignore_exceptions",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--use_density_based_ue",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument(
        "--use_all_ue",
        type=bool,
        default=True,
        action=argparse.BooleanOptionalAction,
    )
    parser.add_argument("--subsample_background_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_train_dataset", type=int, default=-1)
    parser.add_argument("--subsample_eval_dataset", type=int, default=-1)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--seed", nargs="+", type=int)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--save_path", type=str, default="")

    args = parser.parse_args()

    return args

def get_ue_methods(use_all_ue, use_density_based_ue, model_type = None):
    estimators = []
    if use_all_ue:
        estimators += [
                MaximumSequenceProbability(),
                Perplexity(),
                MeanTokenEntropy(),
                MeanPointwiseMutualInformation(),
                MeanConditionalPointwiseMutualInformation(tau=0.0656, lambd=3.599),
                PTrue(),
                PTrueSampling(),
                MonteCarloSequenceEntropy(),
                MonteCarloNormalizedSequenceEntropy(),
                LexicalSimilarity(metric="rouge1"),
                LexicalSimilarity(metric="rouge2"),
                LexicalSimilarity(metric="rougeL"),
                LexicalSimilarity(metric="BLEU"),
                NumSemSets(),
                EigValLaplacian(similarity_score="NLI_score", affinity="entail"),
                EigValLaplacian(similarity_score="NLI_score", affinity="contra"),
                EigValLaplacian(similarity_score="Jaccard_score"),
                DegMat(similarity_score="NLI_score", affinity="entail"),
                DegMat(similarity_score="NLI_score", affinity="contra"),
                DegMat(similarity_score="Jaccard_score"),
                Eccentricity(similarity_score="NLI_score", affinity="entail"),
                Eccentricity(similarity_score="NLI_score", affinity="contra"),
                Eccentricity(similarity_score="Jaccard_score"),
                SemanticEntropy(),
                MaximumTokenProbability(),
                TokenEntropy(),
                PointwiseMutualInformation(),
                ConditionalPointwiseMutualInformation(
                    tau=0.0656, lambd=3.599
                ),  # TODO: What is this ????
                SemanticEntropyToken(model.model_path, args.cache_path),
            ]
    if use_density_based_ue and (model_type is not None):
        if model_type == "Seq2SeqLM":
                estimators += [
                    MahalanobisDistanceSeq("encoder", parameters_path=parameters_path),
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RelativeMahalanobisDistanceSeq(
                        "encoder", parameters_path=parameters_path
                    ),
                    RelativeMahalanobisDistanceSeq(
                        "decoder", parameters_path=parameters_path
                    ),
                    RDESeq("encoder", parameters_path=parameters_path),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("encoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("encoder", md_type="RMD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
            else:
                estimators += [
                    MahalanobisDistanceSeq("decoder", parameters_path=parameters_path),
                    RelativeMahalanobisDistanceSeq(
                        "decoder", parameters_path=parameters_path
                    ),
                    RDESeq("decoder", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="MD", parameters_path=parameters_path),
                    PPLMDSeq("decoder", md_type="RMD", parameters_path=parameters_path),
                ]
    return estimators

def main_run(config, save_path):
    args = get_args() if config is None else config
    save_path = args.save_path if "save_path" in args else save_path

    if args.estimator_module is None:
        estimator_class = globals()[args.estimator]
    else:
        module = importlib.import_module(args.estimator_module)
        estimator_class = getattr(module, args.estimator)
    estimator = estimator_class(**args.estimator_kwargs)
    
    estimators = [estimator]
    estimators += get_ue_methods(args.use_all_ue, args.use_density_based_ue)
        
    if args.seed is None or len(args.seed) == 0:
        args.seed = [1]

    device = args.device
    if device is None:
        device = "cuda:0" if torch.cuda.device_count() > 0 else "cpu"

    for seed in args.seed:
        log.info("=" * 100)
        log.info(f"SEED: {seed}")

        log.info(f"Loading model {args.model}...")
        transformers.set_seed(seed)
        model = WhiteboxModel.from_pretrained(
            args.model,
            device=device,
        )
        log.info("Done with loading model.")

        log.info(f"Loading dataset {args.dataset}...")
        dataset, max_new_tokens = Dataset.load(
            args.dataset,
            args.text_column,
            args.label_column,
            batch_size=args.batch_size,
            prompt=args.prompt,
            split="test",
        )
        
        estimators += get_ue_methods(False, args.use_density_based_ue, model.model_type)

        if any([str(estimator).split('_')[0] in DENSITY_BASED_ESTIMATORS for estimator in all_estimators]):
            if (args.train_dataset is not None) and (
                args.train_dataset != args.dataset
            ):
                train_dataset, _ = Dataset.load(
                    args.train_dataset,
                    args.text_column,
                    args.label_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split="train",
                )
            elif not args.dataset.endswith("csv"):
                train_dataset, _ = Dataset.load(
                    args.train_dataset,
                    args.text_column,
                    args.label_column,
                    batch_size=args.batch_size,
                    prompt=args.prompt,
                    split="train",
                    size=10_000,
                )
            else:
                X_train, X_test, y_train, y_test = dataset.train_test_split(
                    test_size=0.7, seed=seed, split="eval"  # TODO: !
                )
                train_dataset = Dataset(
                    x=X_train, y=y_train, batch_size=args.batch_size
                )
                
            background_train_dataset = Dataset.load(
                args.background_train_dataset,
                "text",
                "url",
                batch_size=args.batch_size,
                data={
                    "data_files": "en/c4-train.00000-of-01024.json.gz"
                },  # TODO: What is this ????
                max_size=100_000,  # TODO:!
                split="train",
            )
            background_train_dataset, _ = Dataset.load(
                    args.background_train_dataset,
                    'text', 'url',
                    batch_size=args.batch_size,
                    size=100_000,
                    split="train",
                    data_files="en/c4-train.00000-of-01024.json.gz",
            )

            if args.subsample_train_dataset != -1:
                train_dataset.subsample(args.subsample_train_dataset, seed=seed)
            if args.subsample_background_train_dataset != -1:
                background_train_dataset.subsample(
                    args.subsample_background_train_dataset, seed=seed
                )

            log.info("Done with loading data.")

            dataset_name = args.dataset.split("/")[-1].split(".")[0]
            model_name = args.model.split("/")[-1]
            parameters_path = f"{args.cache_path}/{dataset_name}/{model_name}"
        else:
            train_dataset = None
            background_train_dataset = None
            density_based_ue = []

        if args.subsample_eval_dataset != -1:
            dataset.subsample(args.subsample_eval_dataset, seed=seed)

        man = UEManager(
            dataset,
            model,
            all_estimators,
            [
                RougeMetric("rouge1"),
                RougeMetric("rouge2"),
                RougeMetric("rougeL"),
                BartScoreSeqMetric("rh"),
                ModelScoreSeqMetric("model_rh"),
                ModelScoreTokenwiseMetric("model_rh"),
                WERTokenwiseMetric(),
                AccuracyMetric()
            ],
            [
                ReversedPairsProportion(),
                PredictionRejectionArea(),
                RiskCoverageCurveAUC(),
            ],
            [
                Logger(),
            ],
            train_data=train_dataset,
            ignore_exceptions=args.ignore_exceptions,
            background_train_data=background_train_dataset,
            max_new_tokens=max_new_tokens,
        )

        man()

        man.save(save_path + f"_seed{seed}")


if "HYDRA_CONFIG" in os.environ:
    hydra_config = Path(os.environ["HYDRA_CONFIG"])

    @hydra.main(
        version_base=None,
        config_path=str(hydra_config.parent),
        config_name=str(hydra_config.name),
    )
    def main_hydra(config):
        output_dir = os.getcwd()
        print(output_dir)
        os.chdir(hydra.utils.get_original_cwd())
        main_run(config, output_dir)

    main = main_hydra

else:
    main = lambda: main_run(None, None)


if __name__ == "__main__":
    main()
