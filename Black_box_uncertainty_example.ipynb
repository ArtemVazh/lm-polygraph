{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvTDkxYoL6eW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pdb\n",
        "import numpy as np\n",
        "from scipy.linalg import eigh\n",
        "import torch.nn as nn\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "def predict(question, model, tokenizer, device = 'cpu', num_beams = 10, num_return_sequences = 10, max_sequence_length = 32):\n",
        "    \"\"\"This function generates sequence = answer on given question\n",
        "\n",
        "    1) it encodes qustion using tokenizer\n",
        "    2) converts tokenized text to device\n",
        "    3) generate answer with dredefined beam size using tokenized text\n",
        "    4) as output we receive answers and their probabilities\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids = tokenizer([question], return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    out = model.generate(input_ids,\n",
        "                         num_return_sequences = num_return_sequences,\n",
        "                         num_beams = num_beams,\n",
        "                         eos_token_id = tokenizer.eos_token_id,\n",
        "                         pad_token_id = tokenizer.pad_token_id,\n",
        "                         output_scores = True,\n",
        "                         return_dict_in_generate=True,\n",
        "                         early_stopping=True,\n",
        "                         max_length=max_sequence_length)\n",
        "\n",
        "    prediction = [tokenizer.decode(out.sequences[i], skip_special_tokens=True) for i in range(num_return_sequences)]\n",
        "\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def compute_jaccard_similarity(lst):\n",
        "    jaccard_sim_mat = np.zeros((len(lst), len(lst)))\n",
        "    scores = []\n",
        "    for i in range(len(lst)):\n",
        "        for j in range(i + 1, len(lst)):\n",
        "            set1 = set(lst[i].lower().split())\n",
        "            set2 = set(lst[j].lower().split())\n",
        "            intersection = len(set1 & set2)\n",
        "            union = len(set1 | set2)\n",
        "            #print(set1, set2, intersection, union,)\n",
        "            jaccard_score = intersection / union\n",
        "\n",
        "            jaccard_sim_mat[i, j] = jaccard_score\n",
        "            #scores.append((lst[i], lst[j], jaccard_score))\n",
        "    return jaccard_sim_mat#scores\n",
        "\n",
        "\n",
        "def get_pairs(lst):\n",
        "    pairs = []\n",
        "    for i in range(len(lst)):\n",
        "        for j in range(i + 1, len(lst)):\n",
        "            pairs.append((lst[i], lst[j], i, j))\n",
        "    return pairs\n",
        "\n",
        "def get_pairs_semsets(lst):\n",
        "    pairs = []\n",
        "    for i in range(len(lst)-1):\n",
        "            pairs.append((lst[i], lst[i+1]))\n",
        "    return pairs\n",
        "\n",
        "def U_Num_Sem_Sets(answers):\n",
        "\n",
        "    lst = get_pairs_semsets(answers)\n",
        "    num_sets = 1\n",
        "\n",
        "    for (sentence_1, sentence_2) in lst:\n",
        "        # Tokenize input sentences\n",
        "        encoded_input_forward = tokenizer_nli(sentence_1, sentence_2, return_tensors='pt')\n",
        "        encoded_input_backward = tokenizer_nli(sentence_2, sentence_1, return_tensors='pt')\n",
        "\n",
        "\n",
        "        logits_forward = model_nli(**encoded_input_forward).logits.detach()\n",
        "        logits_backward = model_nli(**encoded_input_backward).logits.detach()\n",
        "\n",
        "        probs_forward = softmax(logits_forward)\n",
        "        probs_backward = softmax(logits_backward)\n",
        "\n",
        "        a_nli_entail_forward = probs_forward[0][2]\n",
        "        a_nli_entail_backward = probs_backward[0][2]\n",
        "\n",
        "        p_entail_forward = probs_forward[0][2]\n",
        "        p_entail_backward = probs_backward[0][2]\n",
        "\n",
        "        a_nli_contra_forward = 1 - probs_forward[0][0]\n",
        "        a_nli_contra_backward = 1 - probs_backward[0][0]\n",
        "\n",
        "        p_contra_forward = probs_forward[0][0]\n",
        "        p_contra_backward = probs_backward[0][0]\n",
        "\n",
        "        if (p_entail_forward > p_contra_forward) & (p_entail_backward > p_contra_backward):\n",
        "            pass\n",
        "        else:\n",
        "            num_sets += 1\n",
        "\n",
        "        return num_sets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_nli_similarity(lst, order = \"forward\"):\n",
        "    nli_sim_mat_entail, nli_sim_mat_contra = np.zeros((len(lst), len(lst))), np.zeros((len(lst), len(lst)))\n",
        "    nli_prob_mat_entail, nli_prob_mat_contra = np.zeros((len(lst), len(lst))), np.zeros((len(lst), len(lst)))\n",
        "    pairs = get_pairs(lst)\n",
        "    result = []\n",
        "    nli_prob_mat_entail = []\n",
        "    nli_prob_mat_contra = []\n",
        "\n",
        "    j = 1\n",
        "\n",
        "    for i, (sentence_1, sentence_2) in enumerate(pairs):\n",
        "\n",
        "        j=j%len(lst)\n",
        "        print(i, j)\n",
        "\n",
        "        if order == 'forward':\n",
        "            encoded_input = tokenizer_nli(sentence_1, sentence_2, return_tensors='pt')\n",
        "        elif order == 'backward':\n",
        "            encoded_input = tokenizer_nli(sentence_2, sentence_1, return_tensors='pt')\n",
        "\n",
        "        # Perform NLI classification\n",
        "        logits = model_nli(**encoded_input).logits.detach()\n",
        "        probs = softmax(logits)\n",
        "        a_nli_entail = probs[0][2]\n",
        "        p_entail = probs[0][2]\n",
        "\n",
        "        a_nli_contra = 1 - probs[0][0]\n",
        "        p_contra = probs[0][0]\n",
        "\n",
        "        nli_sim_mat_entail[i, j] = a_nli_entail\n",
        "        nli_sim_mat_contra[i, j] = a_nli_contra\n",
        "        # nli_prob_mat_entail.append(p_entail.item())\n",
        "        # nli_prob_mat_contra.append(p_contra.item())\n",
        "\n",
        "        j+=1\n",
        "\n",
        "           #print(sentence_1, sentence_2, a_nli_entail, a_nli_contra, '\\n')\n",
        "\n",
        "    return nli_sim_mat_entail, nli_sim_mat_contra, nli_prob_mat_entail, nli_prob_mat_contra\n",
        "\n",
        "def compute_adjaency_mat(answers):\n",
        "    W = np.eye(len(answers))\n",
        "    pairs = get_pairs(answers)\n",
        "\n",
        "    for (sentence_1, sentence_2, i ,j) in pairs:\n",
        "        # Tokenize input sentences\n",
        "        encoded_input_forward = tokenizer_nli(sentence_1, sentence_2, return_tensors='pt')\n",
        "        encoded_input_backward = tokenizer_nli(sentence_2, sentence_1, return_tensors='pt')\n",
        "\n",
        "\n",
        "        logits_forward = model_nli(**encoded_input_forward).logits.detach()\n",
        "        logits_backward = model_nli(**encoded_input_backward).logits.detach()\n",
        "\n",
        "        probs_forward = softmax(logits_forward)\n",
        "        probs_backward = softmax(logits_backward)\n",
        "\n",
        "        a_nli_entail_forward = probs_forward[0][2]\n",
        "        a_nli_entail_backward = probs_backward[0][2]\n",
        "\n",
        "        # p_entail_forward = probs_forward[0][2]\n",
        "        # p_entail_backward = probs_backward[0][2]\n",
        "\n",
        "        a_nli_contra_forward = 1 - probs_forward[0][0]\n",
        "        a_nli_contra_backward = 1 - probs_backward[0][0]\n",
        "\n",
        "        # p_contra_forward = probs_forward[0][0]\n",
        "        # p_contra_backward = probs_backward[0][0]\n",
        "\n",
        "        w = (a_nli_entail_forward + a_nli_entail_backward) / 2\n",
        "        W[i, j] = w\n",
        "        W[j, i] = w\n",
        "\n",
        "    return W\n",
        "\n",
        "def U_EigVal_Laplacian(answers):\n",
        "    W = compute_adjaency_mat(answers)\n",
        "    D = np.diag(W.sum(axis=1))\n",
        "    D_inverse_sqrt = np.linalg.inv(np.sqrt(D))\n",
        "    L = np.eye(D.shape[0]) - D_inverse_sqrt @ W @ D_inverse_sqrt\n",
        "    return sum([max(0, 1 - lambda_k) for lambda_k in np.linalg.eig(L)[0]])\n",
        "\n",
        "\n",
        "def U_DegMat(answers):\n",
        "    #The Degree Matrix\n",
        "    W = compute_adjaency_mat(answers)\n",
        "    D = np.diag(W.sum(axis=1))\n",
        "    return np.trace(len(answers) - D) / (len(answers) ** 2)\n",
        "\n",
        "def U_Eccentricity(answers, k = 2):\n",
        "\n",
        "    W = compute_adjaency_mat(answers)\n",
        "    D = np.diag(W.sum(axis=1))\n",
        "    D_inverse_sqrt = np.linalg.inv(np.sqrt(D))\n",
        "    L = np.eye(D.shape[0]) - D_inverse_sqrt @ W @ D_inverse_sqrt\n",
        "\n",
        "    # k is hyperparameter  - Number of smallest eigenvectors to retrieve\n",
        "    # Compute eigenvalues and eigenvectors\n",
        "    eigenvalues, eigenvectors = eigh(L)\n",
        "    smallest_eigenvectors = eigenvectors[:, :k]\n",
        "    V_mat = smallest_eigenvectors - smallest_eigenvectors.mean(axis = 0)\n",
        "\n",
        "\n",
        "    norms = np.linalg.norm(V_mat, ord = 2, axis=0)\n",
        "    U_Ecc = np.linalg.norm(norms, 2)\n",
        "    C_Ecc_s_j = norms\n",
        "    return U_Ecc, C_Ecc_s_j\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")\n",
        "\n",
        "# NLI model\n",
        "model_name = \"microsoft/deberta-large-mnli\"\n",
        "model_nli = DebertaForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer_nli = DebertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Questions\n",
        "questions = ['What is the capital of Russia?',\n",
        "             'When Albert Einstein was born?']\n",
        "\n",
        "# Predictions\n",
        "predictions  = [predict_and_proba(question = question, model = model, tokenizer = tokenizer, num_beams=30, num_return_sequences=10) for question in questions]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For example we have the following 3 answers (m = 3) of LLM for a Question - \"What is the capital of Russia\"\n",
        "answers = ['Moscow', 'Olimpia, Greese', 'The capital of Russia is Moscow']\n",
        "\n",
        "U_NumSets = U_Num_Sem_Sets(answers) # number of semantic groups in answer\n",
        "U_EigV = np.round(U_EigVal_Laplacian(answers), 3) # (Due to the Theorem) A continuous analogue of number of semantic sets (higher = bogger uncertainty)\n",
        "U_deg = np.round(U_DegMat(answers), 3) # average pairwise distance (less -> more confident because distance between answers is smaller). Since elems on diag of mat D are sums of similarities between the particular number (position in matrix) and other answers\n",
        "U_Ecc = np.round(U_Eccentricity(answers)[0], 6) # frobenious norm (euclidian norm) between all eigen vectors that are informative embeddings of graph Laplacian (lower this value -> answers are closer in terms of euclidian distance between embeddings = eigenvectors)\n",
        "\n",
        "print(f\"Uncertaimty measures for answers {answers}: \\n\")\n",
        "print(f\"Number of Semantic Sets (U_NumSets) = {U_NumSets}\")\n",
        "print(f\"Sum of Eigenvalues of the Graph Laplacian (U_EigV) = {U_EigV}\")\n",
        "print(f\"The Degree Matrix (U_deg) = {U_deg}\")\n",
        "print(f\"Eccentricity (U_Ecc) = {U_Ecc}\")"
      ],
      "metadata": {
        "id": "vBcPP7ckNHhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For example we have the following 3 answers (m = 3) of LLM for a Question - \"What is the capital of Russia\"\n",
        "answers = ['Moscow', 'Moscow, Russia', 'The capital of Russia is Moscow']\n",
        "\n",
        "U_NumSets = U_Num_Sem_Sets(answers)\n",
        "U_EigV = np.round(U_EigVal_Laplacian(answers), 3)\n",
        "U_deg = np.round(U_DegMat(answers), 3)\n",
        "U_Ecc = np.round(U_Eccentricity(answers)[0], 6)\n",
        "\n",
        "print(f\"Uncertaimty measures for answers {answers}: \\n\")\n",
        "print(f\"Number of Semantic Sets (U_NumSets) = {U_NumSets}\")\n",
        "print(f\"Sum of Eigenvalues of the Graph Laplacian (U_EigV) = {U_EigV}\")\n",
        "print(f\"The Degree Matrix (U_deg) = {U_deg}\")\n",
        "print(f\"Eccentricity (U_Ecc) = {U_Ecc}\")"
      ],
      "metadata": {
        "id": "58uOiGQbYkvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oUcziwOue3eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHIALy3Ce3gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}